{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part I: Prepare our weather training data for the models</h1>\n",
    "<ul>\n",
    "    <li>Need to strip the data for each city in each data set and combine them all into onedataset</li>\n",
    "    <li>Need to also get the data for each city's latitude and longitude and include that in the dataset for predicting the forecasts</li>\n",
    "    <li>Need to prepare labels so that they are offest one hour</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Load data from csv files<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-085287d27125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_weather_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcity_attrib_csv_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"datasets/historical-hourly-weather-data/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcity_attrib_csv_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_weather_data(csv_file):\n",
    "    city_attrib_csv_path = \"datasets/historical-hourly-weather-data/\" + csv_file\n",
    "    return pd.read_csv(city_attrib_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_attributes_csv = load_weather_data(\"city_attributes.csv\")\n",
    "humidity_csv = load_weather_data(\"humidity.csv\")\n",
    "pressure_csv = load_weather_data(\"pressure.csv\")\n",
    "temperature_csv = load_weather_data(\"temperature.csv\")\n",
    "weather_description_csv = load_weather_data(\"weather_description.csv\")\n",
    "wind_direction_csv = load_weather_data(\"wind_direction.csv\")\n",
    "wind_speed_csv = load_weather_data(\"wind_speed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 2: Load the datasets for each city</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_city_dataset(city_name, model_labels_name):\n",
    "    city_dataset = pd.DataFrame(data=humidity_csv[\"datetime\"])\n",
    "    \n",
    "    # create new columns for month, day, hour_of_day\n",
    "    # this will be used to split up data in \"datatime\" column\n",
    "    city_dataset[\"month\"] = \"\"\n",
    "    city_dataset[\"day\"] = \"\"\n",
    "    city_dataset[\"hour_of_day\"] = \"\"\n",
    "    \n",
    "    # seperate the values for datetime into month, day, hour_of_day int columns\n",
    "    for i, date in enumerate(city_dataset[\"datetime\"]):\n",
    "        date, time = date.split(\" \", 1)\n",
    "        year, month, day = date.split(\"-\", 2)\n",
    "        hours, minute_seconds = time.split(\":\", 1)\n",
    "        \n",
    "        city_dataset[\"month\"][i] = int(month)\n",
    "        city_dataset[\"day\"][i] = int(day)\n",
    "        city_dataset[\"hour_of_day\"][i] = int(hours)\n",
    "\n",
    "    # drop datetime column\n",
    "    city_dataset = city_dataset.drop(\"datetime\", axis=1)\n",
    "        \n",
    "    # create new columns for latitude and longitude\n",
    "    # add the values to each row in the table\n",
    "    city_dataset[\"latitude\"] = \"\"\n",
    "    city_dataset[\"longitude\"] = \"\"\n",
    "    city_index = city_attributes_csv.index[city_attributes_csv[\"City\"] == city_name]\n",
    "    latitude_val = city_attributes_csv.get_value(city_index[0], \"Latitude\")\n",
    "    longitude_val = city_attributes_csv.get_value(city_index[0], \"Longitude\")\n",
    "    \n",
    "    for i, row in enumerate(city_dataset[\"day\"]):\n",
    "        city_dataset[\"latitude\"][i] = latitude_val\n",
    "        city_dataset[\"longitude\"][i] = longitude_val\n",
    "    \n",
    "    # create new columns\n",
    "    # assign weather data for the city to columns\n",
    "    city_dataset[\"humidity\"] = humidity_csv[city_name]\n",
    "    city_dataset[\"pressure\"] = pressure_csv[city_name]\n",
    "    city_dataset[\"temperature\"] = temperature_csv[city_name]\n",
    "    city_dataset[\"weather_description\"] = weather_description_csv[city_name]\n",
    "    city_dataset[\"wind_direction\"] = wind_direction_csv[city_name]\n",
    "    city_dataset[\"wind_speed\"] = wind_speed_csv[city_name]\n",
    "    \n",
    "    # create new column for labels\n",
    "    # each label represents the value of an attribute one hour later\n",
    "    city_dataset[model_labels_name + \"_labels\"] = \"\"\n",
    "    for i, row in enumerate(city_dataset[model_labels_name]):\n",
    "        if(i < (len(city_dataset.index) - 1)):\n",
    "            city_dataset[model_labels_name + \"_labels\"][i] = city_dataset[model_labels_name][i + 1]\n",
    "        else:\n",
    "            city_dataset[model_labels_name + \"_labels\"][i] = np.NaN\n",
    "            \n",
    "    return city_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 3: Combine all of the cities into one dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_cities(city_att_file, model_labels_name):\n",
    "    # create new dataset to hold all of the weather data\n",
    "    full_dataset = pd.DataFrame()\n",
    "    \n",
    "    # load data for each city and append to the full dataframe set\n",
    "    for city in city_att_file[\"City\"]:\n",
    "        full_dataset = full_dataset.append(load_city_dataset(city, model_labels_name))\n",
    "    \n",
    "    # re-index the dataframe\n",
    "    full_dataset = full_dataset.reset_index()\n",
    "    full_dataset = full_dataset.drop(columns=['index'])\n",
    "    \n",
    "    return(full_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "full_humidity_dataset = combine_all_cities(city_attributes_csv, model_labels_name=\"humidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>humidity</th>\n",
       "      <th>pressure</th>\n",
       "      <th>temperature</th>\n",
       "      <th>weather_description</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>humidity_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>49.2497</td>\n",
       "      <td>-123.119</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>49.2497</td>\n",
       "      <td>-123.119</td>\n",
       "      <td>76.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>284.630000</td>\n",
       "      <td>mist</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>49.2497</td>\n",
       "      <td>-123.119</td>\n",
       "      <td>76.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>284.629041</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>49.2497</td>\n",
       "      <td>-123.119</td>\n",
       "      <td>76.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>284.626998</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>49.2497</td>\n",
       "      <td>-123.119</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>284.624955</td>\n",
       "      <td>broken clouds</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629103</th>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>31.769</td>\n",
       "      <td>35.2163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629104</th>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>31.769</td>\n",
       "      <td>35.2163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629105</th>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>22</td>\n",
       "      <td>31.769</td>\n",
       "      <td>35.2163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629106</th>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>31.769</td>\n",
       "      <td>35.2163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629107</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>31.769</td>\n",
       "      <td>35.2163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1629108 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        month day hour_of_day latitude longitude  humidity  pressure  \\\n",
       "0          10   1          12  49.2497  -123.119       NaN       NaN   \n",
       "1          10   1          13  49.2497  -123.119      76.0       NaN   \n",
       "2          10   1          14  49.2497  -123.119      76.0       NaN   \n",
       "3          10   1          15  49.2497  -123.119      76.0       NaN   \n",
       "4          10   1          16  49.2497  -123.119      77.0       NaN   \n",
       "...       ...  ..         ...      ...       ...       ...       ...   \n",
       "1629103    11  29          20   31.769   35.2163       NaN       NaN   \n",
       "1629104    11  29          21   31.769   35.2163       NaN       NaN   \n",
       "1629105    11  29          22   31.769   35.2163       NaN       NaN   \n",
       "1629106    11  29          23   31.769   35.2163       NaN       NaN   \n",
       "1629107    11  30           0   31.769   35.2163       NaN       NaN   \n",
       "\n",
       "         temperature weather_description  wind_direction  wind_speed  \\\n",
       "0                NaN                 NaN             NaN         NaN   \n",
       "1         284.630000                mist             0.0         0.0   \n",
       "2         284.629041       broken clouds             6.0         0.0   \n",
       "3         284.626998       broken clouds            20.0         0.0   \n",
       "4         284.624955       broken clouds            34.0         0.0   \n",
       "...              ...                 ...             ...         ...   \n",
       "1629103          NaN                 NaN             NaN         NaN   \n",
       "1629104          NaN                 NaN             NaN         NaN   \n",
       "1629105          NaN                 NaN             NaN         NaN   \n",
       "1629106          NaN                 NaN             NaN         NaN   \n",
       "1629107          NaN                 NaN             NaN         NaN   \n",
       "\n",
       "        humidity_labels  \n",
       "0                    76  \n",
       "1                    76  \n",
       "2                    76  \n",
       "3                    77  \n",
       "4                    78  \n",
       "...                 ...  \n",
       "1629103             NaN  \n",
       "1629104             NaN  \n",
       "1629105             NaN  \n",
       "1629106             NaN  \n",
       "1629107             NaN  \n",
       "\n",
       "[1629108 rows x 12 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_humidity_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataset as csv for easy import on other projects\n",
    "full_humidity_dataset.to_csv('weather_data_with_humidity_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "full_pressure_dataset = combine_all_cities(city_attributes_csv, model_labels_name=\"pressure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pressure_dataset.to_csv('weather_data_with_pressure_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "full_temperature_dataset = combine_all_cities(city_attributes_csv, model_labels_name=\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_temperature_dataset.to_csv('weather_data_with_temperature_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "full_description_dataset = combine_all_cities(city_attributes_csv, \n",
    "                                              model_labels_name=\"weather_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_description_dataset.to_csv('weather_data_with_description_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "full_wdirection_dataset = combine_all_cities(city_attributes_csv,\n",
    "                                            model_labels_name=\"wind_direction\")\n",
    "full_wdirection_dataset.to_csv('weather_data_with_wdirection_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wdirection_dataset.to_csv('weather_data_with_wdirection_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Nick/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "full_wspeed_dataset = combine_all_cities(city_attributes_csv,\n",
    "                                        model_labels_name=\"wind_speed\")\n",
    "full_wspeed_dataset.to_csv('weather_data_with_wspeed_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wspeed_dataset.to_csv('weather_data_with_wspeed_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 4: Clean the full dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_temperature_dataset = load_weather_data('weather_data_with_temperature_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_temperature_dataset = full_temperature_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_temperature_dataset = full_temperature_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 5: Transformer pipeline </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numbers_pipe = Pipeline([\n",
    "     ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "train_num = full_temperature_dataset.drop(\"weather_description\", axis=1)\n",
    "train_num = train_num.drop(\"temperature_labels\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_attribs = list(train_num)\n",
    "cat_attribs = [\"weather_description\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", numbers_pipe, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 6: Split the data into a training and testing set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_labels = full_temperature_dataset[['temperature_labels']]\n",
    "temperature_data = full_temperature_dataset.drop(columns=['temperature_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('num',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('std_scaler',\n",
       "                                                  StandardScaler(copy=True,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True))],\n",
       "                                          verbose=False),\n",
       "                                 ['month', 'day', 'hour_of_day', 'latitude',\n",
       "                                  'longitude', 'humidity', 'pressure',\n",
       "                                  'temperature', 'wind_direction',\n",
       "                                  'wind_speed']),\n",
       "                                ('cat',\n",
       "                                 OneHotEncoder(categories='auto', drop=None,\n",
       "                                               dtype=<class 'numpy.float64'>,\n",
       "                                               handle_unknown='error',\n",
       "                                               sparse=True),\n",
       "                                 ['weather_description'])],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipeline.fit(temperature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "full_temp_training_data, temp_test_data, full_temp_training_labels, temp_test_labels = train_test_split(\n",
    "    temperature_data, temperature_labels)\n",
    "temp_training_data, temp_valid_data, temp_training_labels, temp_valid_labels = train_test_split(\n",
    "    full_temp_training_data, full_temp_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_training_data = full_pipeline.transform(temp_training_data).toarray()\n",
    "temp_training_labels = temp_training_labels.to_numpy()\n",
    "\n",
    "temp_valid_data = full_pipeline.transform(temp_valid_data).toarray()\n",
    "temp_valid_labels = temp_valid_labels.to_numpy()\n",
    "\n",
    "temp_test_data = full_pipeline.transform(temp_test_data).toarray()\n",
    "temp_test_labels = temp_test_labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299298, 64)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_valid_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 7: Start training models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_weather_model(n_hidden=1, n_neurons=30, input_shape=[64]):\n",
    "#    model = keras.models.Sequential()\n",
    "#    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "#    for layer in range(n_hidden):\n",
    "#        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "#    model.add(keras.layers.Dense(1))\n",
    "#    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "#    return model\n",
    "\n",
    "# keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_weather_model)\n",
    "# keras_reg.fit(temp_training_data, temp_training_labels,\n",
    "#             validation_data=(temp_valid_data, temp_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_weather_model)\n",
    "\n",
    "# param_distribs = {\n",
    "#    \"n_hidden\": [0, 1, 2, 3],\n",
    "#    \"n_neurons\": np.arange(1, 100),\n",
    "# }\n",
    "\n",
    "# rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "# rnd_search_cv.fit(temp_training_data, temp_training_labels,\n",
    "#                 validation_data(temp_valid_data, temp_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 897893 samples, validate on 299298 samples\n",
      "Epoch 1/100\n",
      "897893/897893 [==============================] - 29s 33us/sample - loss: 416.2197 - val_loss: 3.1032\n",
      "Epoch 2/100\n",
      "897893/897893 [==============================] - 28s 32us/sample - loss: 2.8143 - val_loss: 2.2343\n",
      "Epoch 3/100\n",
      "897893/897893 [==============================] - 29s 32us/sample - loss: 2.4977 - val_loss: 2.3128\n",
      "Epoch 4/100\n",
      "897893/897893 [==============================] - 30s 34us/sample - loss: 2.3462 - val_loss: 1.9796\n",
      "Epoch 5/100\n",
      "897893/897893 [==============================] - 31s 34us/sample - loss: 2.3046 - val_loss: 3.9064\n",
      "Epoch 6/100\n",
      "897893/897893 [==============================] - 30s 34us/sample - loss: 2.1947 - val_loss: 3.0046\n",
      "Epoch 7/100\n",
      "897893/897893 [==============================] - 31s 34us/sample - loss: 2.1759 - val_loss: 3.0186\n",
      "Epoch 8/100\n",
      "897893/897893 [==============================] - 33s 37us/sample - loss: 2.1142 - val_loss: 2.7551\n",
      "Epoch 9/100\n",
      "897893/897893 [==============================] - 30s 34us/sample - loss: 2.0461 - val_loss: 2.1543\n",
      "Epoch 10/100\n",
      "897893/897893 [==============================] - 30s 33us/sample - loss: 1.9981 - val_loss: 1.7780\n",
      "Epoch 11/100\n",
      "897893/897893 [==============================] - 29s 33us/sample - loss: 1.9478 - val_loss: 1.7894\n",
      "Epoch 12/100\n",
      "897893/897893 [==============================] - 30s 33us/sample - loss: 1.9162 - val_loss: 2.7903\n",
      "Epoch 13/100\n",
      "897893/897893 [==============================] - 32s 36us/sample - loss: 1.8719 - val_loss: 1.7672\n",
      "Epoch 14/100\n",
      "897893/897893 [==============================] - 30s 33us/sample - loss: 1.8317 - val_loss: 1.9423\n",
      "Epoch 15/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.8111 - val_loss: 2.5156\n",
      "Epoch 16/100\n",
      "897893/897893 [==============================] - 25s 28us/sample - loss: 1.7726 - val_loss: 1.5280\n",
      "Epoch 17/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.7449 - val_loss: 1.6246\n",
      "Epoch 18/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.7393 - val_loss: 1.9442\n",
      "Epoch 19/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.7224 - val_loss: 1.5715\n",
      "Epoch 20/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.7161 - val_loss: 1.8390\n",
      "Epoch 21/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.7122 - val_loss: 1.5129\n",
      "Epoch 22/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.6952 - val_loss: 1.7388\n",
      "Epoch 23/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.6770 - val_loss: 1.5140\n",
      "Epoch 24/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.6825 - val_loss: 1.6449\n",
      "Epoch 25/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.6783 - val_loss: 1.5790\n",
      "Epoch 26/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.6529 - val_loss: 1.7942\n",
      "Epoch 27/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.6596 - val_loss: 1.5289\n",
      "Epoch 28/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.6529 - val_loss: 1.7903\n",
      "Epoch 29/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.6401 - val_loss: 1.9377\n",
      "Epoch 30/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.6392 - val_loss: 1.5360\n",
      "Epoch 31/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.6365 - val_loss: 1.4287\n",
      "Epoch 32/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.6261 - val_loss: 1.7660\n",
      "Epoch 33/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.6213 - val_loss: 1.5313\n",
      "Epoch 34/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.6117 - val_loss: 1.8865\n",
      "Epoch 35/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.6115 - val_loss: 1.4885\n",
      "Epoch 36/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.6038 - val_loss: 1.5634\n",
      "Epoch 37/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.5982 - val_loss: 1.9250\n",
      "Epoch 38/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.6042 - val_loss: 1.5049\n",
      "Epoch 39/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.5887 - val_loss: 1.5842\n",
      "Epoch 40/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.5929 - val_loss: 1.3844\n",
      "Epoch 41/100\n",
      "897893/897893 [==============================] - 27s 31us/sample - loss: 1.5845 - val_loss: 1.4775\n",
      "Epoch 42/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.5784 - val_loss: 1.4588\n",
      "Epoch 43/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.5750 - val_loss: 1.7280\n",
      "Epoch 44/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.5678 - val_loss: 1.5293\n",
      "Epoch 45/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.5681 - val_loss: 1.4772\n",
      "Epoch 46/100\n",
      "897893/897893 [==============================] - 27s 30us/sample - loss: 1.5677 - val_loss: 1.5218\n",
      "Epoch 47/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.5550 - val_loss: 1.5694\n",
      "Epoch 48/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.5586 - val_loss: 2.6826\n",
      "Epoch 49/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.5451 - val_loss: 1.3860\n",
      "Epoch 50/100\n",
      "897893/897893 [==============================] - 26s 29us/sample - loss: 1.5556 - val_loss: 1.4426\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = build_weather_model(n_hidden=4, n_neurons=150, input_shape=temp_training_data.shape[1:])\n",
    "weather_model = model.fit(temp_training_data, temp_training_labels, epochs=100,\n",
    "                   batch_size=128, validation_data=(temp_valid_data, temp_valid_labels),\n",
    "                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64864/399064 [===>..........................] - ETA: 11s - loss: 2.3235"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-308-fcdd598cb11f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#1.4517\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmse_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_test_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_test_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#1.4517\n",
    "mse_test = model.evaluate(temp_test_data, temp_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = temp_test_data[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[278.3897 ],\n",
       "       [287.9649 ],\n",
       "       [295.60535]], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[279.17 ],\n",
       "       [286.218],\n",
       "       [296.13 ]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_test_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
